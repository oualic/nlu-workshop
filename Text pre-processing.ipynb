{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to load the package you are going to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB dataset contains movie reviews along with their associated binary sentiment polarity labels. It is intended to serve as a benchmark for sentiment classification. The core dataset contains 50,000 reviews split evenly into 25k train and 25k test sets. \n",
    "\n",
    "The dataset can be dowloaded from http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "The dataset is introduced in this paper https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the IMDB dataset from df_IMDb.csv file. Note that the dataset is already converted from the orginal source format. \n",
    "\n",
    "Using the head() method we print the first 5 rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv('data/small_df_IMDb.csv')\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} observations and {} classes in this dataset. \\n\".format(df_reviews.shape[0],df_reviews.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look closer to one of these reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.loc[12]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper into IMBD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordCloud (https://amueller.github.io/word_cloud/index.html) is a technique to show which words are the most frequent among the given text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_reviews.loc[100]['review']\n",
    "\n",
    "#  Create stopword list\n",
    "my_stopwords = set(STOPWORDS)\n",
    "my_stopwords.update([\"br\"])\n",
    "\n",
    "# generate and save the word cloud image to a file\n",
    "wc = WordCloud(scale=5, \n",
    "               background_color=\"white\", \n",
    "               max_words=100, \n",
    "               stopwords=my_stopwords)\n",
    "\n",
    "wc.generate(text)\n",
    "wc.to_file(\"WordCloud.png\")\n",
    "\n",
    "# show the wordcloud as output\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will define a function that takes a text string as a parameter and then performs preprocessing on the string to remove special characters and HTML tags from the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numbers removing\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "def remove_numbers(input_text):\n",
    "    return re.sub(r'\\d+', '', input_text)\n",
    "\n",
    "def remove_punctuation(input_text):\n",
    "    return ''.join(c for c in input_text if c not in punctuation)\n",
    "\n",
    "def remove_tags(text):\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def remove_multiple_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "this text.   should be cleaned. \n",
    "there are mainly 3 steps, (1) remove      numbers, (2) remove punctuation (3) remove tags like <b>.\n",
    "We also need to remove symbols like &^@....'\n",
    "\"\"\"  \n",
    "\n",
    "# text = df_reviews.loc[150]['review']\n",
    "\n",
    "print('----- Original text', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n*** Remove tags')\n",
    "text = remove_tags(text)\n",
    "print(text)\n",
    "\n",
    "print('\\n*** Remove numbers')\n",
    "text = remove_numbers(text)\n",
    "print(text)\n",
    "\n",
    "print('\\n*** Remove puctuation')\n",
    "text = remove_punctuation(text)\n",
    "print(text)\n",
    "\n",
    "print('\\n*** Remove multiple spaces')\n",
    "text = remove_multiple_spaces(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>“Stop words”</b> are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts. It is possible to remove stop words using Natural Language Toolkit (NLTK), a suite of libraries and programs for symbolic and statistical natural language processing.\n",
    "\n",
    "<b>Tokenization</b> is the process of taking a text or set of texts and breaking it up into its individual words. In this step, we will tokenize text with the help of splitting text by space or punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize(text):\n",
    "    stop_words = ENGLISH_STOP_WORDS\n",
    "    tokens = word_tokenize(text)\n",
    "    result = [i for i in tokens if not i in stop_words]\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing inflected words to their word stem. For example, \"listen\", \"listened\", \"listening\" are reduced to the same stem \"listen\". Some application like sentiment analysis can benefit from stemming because it reduces vocabulary and increase the relevance of the concept.\n",
    "\n",
    "For stemming, we can use SnowballStemmer in nltk.stem.snowball:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "st = SnowballStemmer('english')\n",
    "\n",
    "print('running -->',st.stem('running'))\n",
    "print('greatly -->', st.stem('greatly'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. For example, \"am\", \"are\", \"is\" are lemmatized to the same form \"be\".\n",
    "\n",
    "For stemming, we can use WordNetLemmatizer in nltk.stem.wordnet. Before we use the lemmatizer, we should download WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('am', pos='v'))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
