{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec on Amazon review corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a very popular Natural Language Processing technique nowadays that uses a neural network to learn the vector representations of words called “word embeddings” in a particular text.\n",
    "\n",
    "In this tutorial, we will use the excellent implementation of word2vec from the gensimpackage to build our word2vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Amazon review corpus on Health and Personal Care. The dataset is in json format and contains 346,355 reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to combine all the reviews text of the dataset into one string. For that, we will first use Pandas to load the dataset. We can do so with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import gzip\n",
    "import pandas as pd\n",
    "url = 'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Health_and_Personal_Care_5.json.gz'\n",
    "\n",
    "urllib.request.urlretrieve(url, 'data/review.json.gz')\n",
    "\n",
    "with gzip.open('data/review.json.gz') as f:\n",
    "    raw_df = pd.read_json(f, lines=True)\n",
    "\n",
    "print(\"Data loaded\")\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset is too large, and will take a lot of tim for training. Let's reduce its size:\n",
    "raw_df = raw_df.loc[0:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the DataFrame has many columns but we are only interested in the column that contains the review text. That column name is reviewText. We will use Python join method on that column to combine all the reviews text in one string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the review text into a long string and print its length\n",
    "raw_corpus = u\"\".join(raw_df['reviewText']+\" \")\n",
    "print(\"Raw Corpus contains {0:,} characters\".format(len(raw_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim’s word2vec expects a sequence of sentences as its input, each one as a list of words. We then need to split the string that we obtain in the above section into sentences. For that, we will use NLTK‘s punkt tokenizer for sentence splitting. In order to use this, we will need to install NLTK and download the relevant training file for punkt. The following code help to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import natural language toolkit\n",
    "import nltk\n",
    "# download the punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "print(\"The punkt tokenizer is downloaded\")\n",
    " \n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "print(\"The punkt tokenizer is loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load the punkt tokenizer and use it to split our very long string into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "print(\"The punkt tokenizer is loaded\")\n",
    "# we tokenize the raw string into raw sentences\n",
    "raw_sentences = tokenizer.tokenize(raw_corpus)\n",
    "print(\"We have {0:,} raw sentences\".format(len(raw_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenization, we see that our dataset contains 1,824,643 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and split sentence into words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to clean the sentences we get after tokenization. The cleaning consists to removes punctuation, parentheses, question marks, etc., and leaves only alphabetic character. Also gensim’s word2vec expects each sentence to be a list of words. So we need to convert each clean sentence to a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Clean and split sentence into words\n",
    "def clean_and_split_str(string):\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z]+\")\n",
    "    string = re.sub(strip_special_chars, \" \", string)\n",
    "    return string.strip().split()\n",
    "     \n",
    "# clean each raw sentences and build the list of sentences\n",
    "sentences = []\n",
    "for raw_sent in raw_sentences:\n",
    "    if len(raw_sent) > 0:\n",
    "        sentences.append(clean_and_split_str(raw_sent))\n",
    "print(\"We have {0:,} clean sentences\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code help to count the number of token present in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The dataset corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the numerical parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim’s Word2Vec API accepts several parameters that affect both training speed and quality. The important parameters are:\n",
    "\n",
    "* size :It is the dimensionality of the resulting word vectors\n",
    "* min_count : It is the minimum word count threshold. In order words, we ignore all words with total frequency lower than this.\n",
    "* workers : It is the number of threads to run in parallel.\n",
    "* window : It is the maximum distance between the current and predicted word within a sentence.\n",
    "* seed : it is for the RNG, to make the result reproducible.\n",
    "\n",
    "The following code shows the value of the parameters of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    " \n",
    "#Dimensionality of the resulting word vectors\n",
    "num_features = 200\n",
    " \n",
    "#Minimum word count threshold\n",
    "min_word_count = 3\n",
    " \n",
    "#Number of threads to run in parallel\n",
    "num_workers = multiprocessing.cpu_count()\n",
    " \n",
    "#Context window length\n",
    "context_size = 7\n",
    " \n",
    "#Seed for the RNG, to make the result reproducible\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the parameters above to initialize our word2vec model. After the initialisation, we will fisrt build the vocabulary of our dataset. After that, we will train our word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    " \n",
    "word2vec_model = gensim.models.word2vec.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers, \n",
    "    size=num_features, \n",
    "    min_count=min_word_count, \n",
    "    window=context_size)\n",
    "     \n",
    "word2vec_model.build_vocab(sentences=sentences)\n",
    "print(\"The vocabulary is built\")\n",
    "print(\"Word2Vec vocabulary length: \", len(word2vec_model.wv.vocab))\n",
    " \n",
    "#Start training the model\n",
    "print('model is tranining!')\n",
    "word2vec_model.train(sentences=sentences, total_examples=word2vec_model.corpus_count, epochs=4)\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we will now save it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "word2vec_model.save(\"data/trained_word2vec_model.w2v\")\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load our model with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our word2vec model\n",
    "import gensim\n",
    "w2v_model = gensim.models.word2vec.Word2Vec.load(\"data/trained_word2vec_model.w2v\")\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training, we can visualize the learned embeddings using t-SNE. t-SNE is a tool for data visualization that reduces the dimensionality of data to 2 or 3 dimensions so that it can be plotted easily. Because the space complexity of the t-SNE algorithm is quadratic, in this tutorial we will view only a part of our model. We use the following code to select 10,000 words from our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "count = 10000\n",
    "word_vectors_matrix = np.ndarray(shape=(count, 200), dtype='float64')\n",
    "word_list = []\n",
    "i = 0\n",
    "for word in w2v_model.wv.vocab:\n",
    "    word_vectors_matrix[i] = w2v_model[word]\n",
    "    word_list.append(word)\n",
    "    i = i+1\n",
    "    if i == count:\n",
    "        break\n",
    "print(\"word_vectors_matrix shape is \", word_vectors_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now initialize model and compress our word vectors into 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compress the word vectors into 2D space\n",
    "import sklearn.manifold\n",
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)\n",
    "word_vectors_matrix_2d = tsne.fit_transform(word_vectors_matrix)\n",
    "print(\"word_vectors_matrix_2d shape is \", word_vectors_matrix_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a Pandas Dataframe that contains the selected words and the x and y coordinates of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = pd.DataFrame(\n",
    "    [\n",
    "        (word, coords[0], coords[1]) \n",
    "        for word, coords in [\n",
    "            (word, word_vectors_matrix_2d[word_list.index(word)])\n",
    "            for word in word_list\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\"word\", \"x\", \"y\"]\n",
    ")\n",
    "print(\"Points DataFrame built\")\n",
    "points.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the Points DataFrame to plot our words vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_context(\"poster\")\n",
    "points.plot.scatter(\"x\", \"y\", s=10, figsize=(20, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will zoom in to some region in order to see the similarity of the words. We create a function that create a bounding box of x and y coordinates and plot only the words between that bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_region(x_bounds, y_bounds):\n",
    "    slice = points[\n",
    "        (x_bounds[0] <= points.x) &\n",
    "        (points.x <= x_bounds[1]) &\n",
    "        (y_bounds[0] <= points.y) &\n",
    "        (points.y <= y_bounds[1]) \n",
    "    ]\n",
    "    \n",
    "    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n",
    "    for i, point in slice.iterrows():\n",
    "        ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_region(x_bounds=(-15.0, -2.0), y_bounds=(-15.05, -2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, words that are similar end up clustering nearby each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to check if we have a good word2vec model is to use the model to find the most similar words to a specific word. For that, we can use the most_similarfunction that returns the 10 most similar words to the given word. Let’s find the most similar words to the word blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.most_similar(\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
