{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the dataset from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/couali/Desktop/ai-workshop/other/cs230-code-examples/pytorch/nlp/test'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Kaggle dataset into memory...\n",
      "- done.\n",
      "Saving in ../data/kaggle/train...\n",
      "- done.\n",
      "Saving in ../data/kaggle/val...\n",
      "- done.\n",
      "Saving in ../data/kaggle/test...\n",
      "- done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Read, split and save the kaggle dataset for our model\"\"\"\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def load_dataset(path_csv):\n",
    "    \"\"\"Loads dataset into memory from csv file\"\"\"\n",
    "    # Open the csv file, need to specify the encoding for python3\n",
    "    use_python3 = sys.version_info[0] >= 3\n",
    "    with (open(path_csv, encoding=\"windows-1252\") if use_python3 else open(path_csv)) as f:\n",
    "        csv_file = csv.reader(f, delimiter=',')\n",
    "        dataset = []\n",
    "        words, tags = [], []\n",
    "\n",
    "        # Each line of the csv corresponds to one word\n",
    "        for idx, row in enumerate(csv_file):\n",
    "            if idx == 0: continue\n",
    "            sentence, word, pos, tag = row\n",
    "            # If the first column is non empty it means we reached a new sentence\n",
    "            if len(sentence) != 0:\n",
    "                if len(words) > 0:\n",
    "                    assert len(words) == len(tags)\n",
    "                    dataset.append((words, tags))\n",
    "                    words, tags = [], []\n",
    "            try:\n",
    "                word, tag = str(word), str(tag)\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(\"An exception was raised, skipping a word: {}\".format(e))\n",
    "                pass\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def save_dataset(dataset, save_dir):\n",
    "    \"\"\"Writes sentences.txt and labels.txt files in save_dir from dataset\n",
    "    Args:\n",
    "        dataset: ([([\"a\", \"cat\"], [\"O\", \"O\"]), ...])\n",
    "        save_dir: (string)\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    print(\"Saving in {}...\".format(save_dir))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Export the dataset\n",
    "    with open(os.path.join(save_dir, 'sentences.txt'), 'w') as file_sentences:\n",
    "        with open(os.path.join(save_dir, 'labels.txt'), 'w') as file_labels:\n",
    "            for words, tags in dataset:\n",
    "                file_sentences.write(\"{}\\n\".format(\" \".join(words)))\n",
    "                file_labels.write(\"{}\\n\".format(\" \".join(tags)))\n",
    "    print(\"- done.\")\n",
    "\n",
    "\n",
    "# Check that the dataset exists (you need to make sure you haven't downloaded the `ner.csv`)\n",
    "path_dataset = '../data/kaggle/ner_dataset.csv'\n",
    "msg = \"{} file not found. Make sure you have downloaded the right dataset\".format(path_dataset)\n",
    "assert os.path.isfile(path_dataset), msg\n",
    "\n",
    "# Load the dataset into memory\n",
    "print(\"Loading Kaggle dataset into memory...\")\n",
    "dataset = load_dataset(path_dataset)\n",
    "print(\"- done.\")\n",
    "\n",
    "# Split the dataset into train, val and split (dummy split with no shuffle)\n",
    "train_dataset = dataset[:int(0.7*len(dataset))]\n",
    "val_dataset = dataset[int(0.7*len(dataset)) : int(0.85*len(dataset))]\n",
    "test_dataset = dataset[int(0.85*len(dataset)):]\n",
    "\n",
    "# Save the datasets to files\n",
    "save_dataset(train_dataset, '../data/kaggle/train')\n",
    "save_dataset(val_dataset, '../data/kaggle/val')\n",
    "save_dataset(test_dataset, '../data/kaggle/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/kaggle/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The step will allow the facility to operate at full capacity .\n",
      "O O O O O O O O O O O O\n",
      "33571  examples\n"
     ]
    }
   ],
   "source": [
    "with open(data_path+'sentences.txt', 'r') as f:\n",
    "    sentences = f.read().split('\\n')\n",
    "with open(data_path+'labels.txt', 'r') as f:\n",
    "    sentence_tags = f.read().split('\\n')\n",
    "\n",
    "print(sentences[10])\n",
    "print(sentence_tags[10])\n",
    "print(len(sentences), ' examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =[sent.split(' ') for sent in sentences]\n",
    "sentence_tags =[sent.split(' ') for sent in sentence_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrats will now have a better chance to protect or expand their one-seat majority in the Senate .\n",
      "O O O O O O O O O O O O O O O O B-org O\n"
     ]
    }
   ],
   "source": [
    "print(sentences[193])\n",
    "print(sentence_tags[193])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences[9]), len(sentence_tags[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sentences = np.array(sentences)\n",
    "sentence_tags = np.array(sentence_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    " \n",
    "(train_sentences, \n",
    "test_sentences, \n",
    "train_tags, \n",
    "test_tags) = train_test_split(sentences, sentence_tags, test_size=0.1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "\n",
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    "\n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13167, 11174, 2486, 9148, 5586, 5494, 2997, 7739, 3783, 6357, 7069, 5195, 9223, 22448, 2486, 12219, 6373, 22137, 24493, 12219, 5105, 22612, 8229, 9223, 2178, 5825, 24493, 263, 12931]\n",
      "[12219, 2905, 10963, 14864, 12917, 23478, 17878, 18798, 20604, 21601, 19457, 15493, 5105, 25290, 2178, 20604, 13949, 12931]\n",
      "[11, 11, 11, 16, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
      "[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    " \n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    " \n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    " \n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    " \n",
    "for s in test_tags:\n",
    "    test_tags_y.append([tag2index[t] for t in s])\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(MAX_LENGTH)  # 271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13167 11174  2486  9148  5586  5494  2997  7739  3783  6357  7069  5195\n",
      "  9223 22448  2486 12219  6373 22137 24493 12219  5105 22612  8229  9223\n",
      "  2178  5825 24493   263 12931     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "[12219  2905 10963 14864 12917 23478 17878 18798 20604 21601 19457 15493\n",
      "  5105 25290  2178 20604 13949 12931     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "[11 11 11 16 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n",
      "[11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 104, 128)          3319680   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 104, 512)          788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 104, 19)           9747      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 104, 19)           0         \n",
      "=================================================================\n",
      "Total params: 4,117,907\n",
      "Trainable params: 4,117,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "print(cat_train_tags_y[0])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24170 samples, validate on 6043 samples\n",
      "Epoch 1/1\n",
      "24170/24170 [==============================] - 121s 5ms/step - loss: 0.1324 - acc: 0.9692 - val_loss: 0.0865 - val_acc: 0.9743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd493d34ac8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3358/3358 [==============================] - 7s 2ms/step\n",
      "acc: 98.71575342820755\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   # acc: 99.09751977804825\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['At', 'the', 'Group', 'of', 'Eight', 'summit', 'in', 'Scotland', ',', 'Japanese', 'Prime', 'Minister', 'Junichiro', 'Koizumi', 'said', 'he', 'is', 'outraged', 'by', 'the', 'London', 'attacks', '.He', 'noted', 'terrorist', 'acts', 'must', 'not', 'be', 'forgivable', '.'], ['Sarin', 'gas', 'attacks', 'on', 'the', 'Tokyo', 'subway', 'system', 'in', '1995', 'killed', '12', 'people', 'and', 'injured', 'thousands', '.']]\n"
     ]
    }
   ],
   "source": [
    "test_samples = [\n",
    "    \"At the Group of Eight summit in Scotland , Japanese Prime Minister Junichiro Koizumi said he is outraged by the London attacks .He noted terrorist acts must not be forgivable . \".split(),\n",
    "    \"Sarin gas attacks on the Tokyo subway system in 1995 killed 12 people and injured thousands .\".split(),\n",
    "    \n",
    "]\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19324 12219 17976 24493  7401 21401  2486 17131 22612 21191  7969  7033\n",
      "  22505  1107 25276 12427 20988 23251 13153 12219 10478   624     1 11613\n",
      "  21814  7506 12131  7581 20604     1 12931     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    1  2611   624  6768 12219 21930  5271 25414  2486  6043  4212 24558\n",
      "  23143  7739 24340 20856 12931     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "test_samples_X = []\n",
    "for s in test_samples:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    test_samples_X.append(s_int)\n",
    " \n",
    "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
    "print(test_samples_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.30730506e-04 2.70952191e-02 1.51855536e-04 ... 2.98023410e-02\n",
      "   1.23702004e-04 1.37163370e-04]\n",
      "  [1.50635271e-04 2.47660521e-02 1.41025623e-04 ... 2.72389241e-02\n",
      "   1.16164381e-04 1.27625288e-04]\n",
      "  [2.07328048e-04 3.29984613e-02 1.91689513e-04 ... 3.90817411e-02\n",
      "   1.65869395e-04 1.78201371e-04]\n",
      "  ...\n",
      "  [1.00000000e+00 2.18545963e-11 1.02937203e-09 ... 5.35006872e-10\n",
      "   3.05242664e-10 5.91202087e-10]\n",
      "  [1.00000000e+00 2.90565298e-11 1.84811366e-09 ... 6.71890377e-10\n",
      "   5.35562261e-10 1.04345066e-09]\n",
      "  [1.00000000e+00 4.12342174e-11 3.29064354e-09 ... 9.17855680e-10\n",
      "   9.29676391e-10 1.81660753e-09]]\n",
      "\n",
      " [[1.61607997e-04 3.70741487e-02 2.04422424e-04 ... 4.51217741e-02\n",
      "   1.75203968e-04 1.90763574e-04]\n",
      "  [1.29431326e-04 2.56545246e-02 1.46122664e-04 ... 3.11382655e-02\n",
      "   1.17775424e-04 1.30371976e-04]\n",
      "  [1.45706101e-04 2.25255229e-02 1.35804468e-04 ... 2.79560611e-02\n",
      "   1.08024717e-04 1.17837269e-04]\n",
      "  ...\n",
      "  [1.00000000e+00 2.13960290e-11 1.01572173e-09 ... 5.29156219e-10\n",
      "   3.02887465e-10 5.81090176e-10]\n",
      "  [1.00000000e+00 2.84799632e-11 1.82480309e-09 ... 6.65011990e-10\n",
      "   5.31634736e-10 1.02653697e-09]\n",
      "  [1.00000000e+00 4.04603608e-11 3.25117133e-09 ... 9.09058995e-10\n",
      "   9.23194965e-10 1.78869608e-09]]] (2, 104, 19)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = model.predict(test_samples_X)\n",
    "print(predictions, predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    " \n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 104, 128)          3319680   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 104, 512)          788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 104, 19)           9747      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 104, 19)           0         \n",
      "=================================================================\n",
      "Total params: 4,117,907\n",
      "Trainable params: 4,117,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24170 samples, validate on 6043 samples\n",
      "Epoch 1/3\n",
      "24170/24170 [==============================] - 129s 5ms/step - loss: 0.0888 - acc: 0.9750 - ignore_accuracy: 0.8811 - val_loss: 0.0644 - val_acc: 0.9823 - val_ignore_accuracy: 0.9168\n",
      "Epoch 2/3\n",
      "24170/24170 [==============================] - 122s 5ms/step - loss: 0.0463 - acc: 0.9872 - ignore_accuracy: 0.9391 - val_loss: 0.0431 - val_acc: 0.9880 - val_ignore_accuracy: 0.9433\n",
      "Epoch 3/3\n",
      "24170/24170 [==============================] - 121s 5ms/step - loss: 0.0307 - acc: 0.9913 - ignore_accuracy: 0.9585 - val_loss: 0.0365 - val_acc: 0.9896 - val_ignore_accuracy: 0.9511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd437ee38d0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'O', 'O', 'O', 'I-org', 'O', 'O', 'B-geo', 'O', 'B-gpe', 'B-per', 'I-per', 'I-per', 'I-per', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_samples_X)\n",
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Sarin',\n",
       "  'gas',\n",
       "  'attacks',\n",
       "  'on',\n",
       "  'the',\n",
       "  'Tokyo',\n",
       "  'subway',\n",
       "  'system',\n",
       "  'in',\n",
       "  '1995',\n",
       "  'killed',\n",
       "  '12',\n",
       "  'people',\n",
       "  'and',\n",
       "  'injured',\n",
       "  'thousands',\n",
       "  '.'],)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"At the Group of Eight summit in Scotland , Japanese Prime Minister Junichiro Koizumi said he is outraged by the London attacks .He noted terrorist acts must not be forgivable . \".split(),\n",
    "    \"Sarin gas attacks on the Tokyo subway system in 1995 killed 12 people and injured thousands .\".split(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yourika-env]",
   "language": "python",
   "name": "conda-env-yourika-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
